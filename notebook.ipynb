{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d4387f-fa24-4a80-906b-4cca195b6736",
   "metadata": {},
   "source": [
    "# Data Scientist Professional Practical Exam Submission\n",
    "\n",
    "**Use this template to write up your summary for submission. Code in Python or R needs to be included.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d9d77-2e1e-4523-a28e-1e3b388f930c",
   "metadata": {},
   "source": [
    "## ðŸ“ Task List\n",
    "\n",
    "Your written report should include both code, output and written text summaries of the following:\n",
    "- Data Validation:   \n",
    "  - Describe validation and cleaning steps for every column in the data \n",
    "- Exploratory Analysis:  \n",
    "  - Include two different graphics showing single variables only to demonstrate the characteristics of data  \n",
    "  - Include at least one graphic showing two or more variables to represent the relationship between features\n",
    "  - Describe your findings\n",
    "- Model Development\n",
    "  - Include your reasons for selecting the models you use as well as a statement of the problem type\n",
    "  - Code to fit the baseline and comparison models\n",
    "- Model Evaluation\n",
    "  - Describe the performance of the two models based on an appropriate metric\n",
    "- Business Metrics\n",
    "  - Define a way to compare your model performance to the business\n",
    "  - Describe how your models perform using this approach\n",
    "- Final summary including recommendations that the business should undertake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2018abd-bb40-4fe4-afcd-0646fa327d2f",
   "metadata": {},
   "source": [
    "**Importing relevant packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45554a01-416e-4127-8e3e-53cc0290cb5a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1759768148438,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Start coding here...\nfrom scipy.stats import zscore, chi2_contingency, f_oneway\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    precision_score,\n    recall_score,\n    roc_auc_score,\n)\nimport pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT REQUIRED LIBRARIES AND MODULES\n",
    "# ============================================================================\n",
    "# Standard scientific computing libraries\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model_development import LogisticRegressionModel, RandomForestModel\n",
    "from model_evaluation import ModelEvaluator\n",
    "from utils import DataExplorer, FeatureEngineering, ModelInterpreter, StatisticalTester\n",
    "from validation import DataValidator\n",
    "from visualization import DataVisualizer, ModelVisualizer\n",
    "\n",
    "# Suppress all warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize explorers and handlers for use throughout notebook\n",
    "explorer = DataExplorer()\n",
    "fe = FeatureEngineering()\n",
    "visualizer = DataVisualizer()\n",
    "model_viz = ModelVisualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e74c5-3a79-45f6-9f2d-c7e7fb970ff4",
   "metadata": {},
   "source": [
    "# Data Import, validation, and cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae3761-d973-49b3-8fe4-a124a6bfafaf",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1759768148551,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load the data and data summary and description\ndf = pd.read_csv(\"recipe_site_traffic_2212.csv\")\nprint(\"=== DataFrame Info ===\")\nprint(df.info())\nprint(\"\\n=== General Stats for All Columns ===\")\nprint(df.describe(include=\"all\").T)",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD AND EXAMINE DATA\n",
    "# ============================================================================\n",
    "# Define column names for validation and analysis\n",
    "num_cols = [\"calories\", \"carbohydrate\", \"sugar\", \"protein\"]\n",
    "cat_cols = [\"category\", \"servings\"]\n",
    "id_col = \"recipe\"\n",
    "target_col = \"high_traffic\"\n",
    "\n",
    "# Load the recipe traffic dataset from CSV file and display examination info\n",
    "df = explorer.load_and_examine_data(\"recipe_site_traffic_2212.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0262849b-3ca3-4ae4-bab6-347ecef68ad8",
   "metadata": {},
   "source": [
    "**Initial comments**\n",
    "- There are 947 entries\n",
    "- There are many missings in the column high_traffic, but given the assignment it might be that only observation with high traffic have been coded in the high_traffic column. This intuition is supported by the fact that there is only 1 unique value. It deserves further investigation.\n",
    "- There are 52 missings in the numerical columns (calories, carbohydrate, sugar, protein). They all have same number of missings, they might be observation with completely missing information over those columns. It deserves further investigation\n",
    "- Good news: no missings in the identifier column named recipe, minimum is 1 and max 947 with int dtype. It looks a legitimate identifier column, but I will test if each recipe has unique number.\n",
    "- The column category should have 10 unique values, it has 11. It will be furtherly investigated.\n",
    "- The column servings should be numerical type (float or int), but it is object type, probably mixed dtypes in the column. It will be investigated.\n",
    "- The column servings has only limited amount of unique values even though officially should be numerical. It will be considered as categorical in the analysis for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911d964-f3ef-4022-9227-65ff2e1960a2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1759768148598,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Set column lists for validation\nnum_cols = [\"calories\", \"carbohydrate\", \"sugar\", \"protein\"]\ncat_cols = [\"category\", \"servings\"]\nid_col = \"recipe\"\ntarget_col = \"high_traffic\""
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHECK FOR MISSING VALUES AND DATA INTEGRITY\n",
    "# ============================================================================\n",
    "# Check for missing values and verify data integrity\n",
    "explorer.check_data_integrity(df, id_col=id_col, target_col=target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb274ef-7719-4d5d-b516-c438602c4bee",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1759768148649,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Missingness, check for uniqueness of the identifier column recipe and for the target column high_traffic\nprint(\"\\n=== Missing Values ===\")\nprint(df.isnull().sum())\nunique_ids = df[id_col].nunique()\ntotal_ids = len(df[id_col])\nprint(f\"\\nUnique IDs: {unique_ids}, Total: {total_ids}\")\nif unique_ids != total_ids:\n    print(\"WARNING: Recipe IDs are not unique!\")\nprint(f\"\\nUnique {target_col} values:\")\nprint(df[target_col].unique())\nprint(df[target_col].value_counts())",
    "outputsMetadata": {
     "0": {
      "height": 395,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHECK FOR MISSING VALUES AND DATA INTEGRITY\n",
    "# ============================================================================\n",
    "# Define column names for later use throughout the analysis\n",
    "num_cols = [\"calories\", \"carbohydrate\", \"sugar\", \"protein\"]\n",
    "cat_cols = [\"category\", \"servings\"]\n",
    "id_col = \"recipe\"\n",
    "target_col = \"high_traffic\"\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Verify that recipe identifiers are unique\n",
    "unique_ids = df[id_col].nunique()\n",
    "total_ids = len(df[id_col])\n",
    "print(f\"\\nUnique IDs: {unique_ids}, Total: {total_ids}\")\n",
    "if unique_ids != total_ids:\n",
    "    print(\"WARNING: Recipe IDs are not unique!\")\n",
    "\n",
    "# Examine the target variable\n",
    "print(f\"\\nUnique {target_col} values:\")\n",
    "print(df[target_col].unique())\n",
    "print(df[target_col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62845b-7290-4fc8-8ee8-9ba3a6308918",
   "metadata": {},
   "source": [
    "**Detecting missings**\n",
    "- The recipe column (which is the identifier column) has 947 unique int values for 947 entries, with minimum 1 and max 947. It is a legitimate identifier column. \n",
    "- The count of missing for calories, carbohydrate, sugar and protein is confirmed. Further analysis on the outliers will come afterwards.\n",
    "- 373 Missings for the target column high_traffic, 1 unique value which is \"High\". The values of the column high_traffic has been coded as \"High\" only when the observation leads to higher web traffic, in all other cases it is missing. The column will be changed ahead to a boolean column with value True if the current value is \"High\" and to False when it is missing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9668c2c2-8017-4a93-9106-1ce3d316cd41",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1759768148699,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Categorical column: unique values & value counts\nprint(\"\\nUnique category values:\")\nprint(df[\"category\"].unique())\nprint(df[\"category\"].value_counts())\n\n# Category validation against expected set\nexpected_categories = set(\n    [\n        \"Lunch/Snacks\",\n        \"Beverages\",\n        \"Potato\",\n        \"Vegetable\",\n        \"Meat\",\n        \"Chicken\",\n        \"Pork\",\n        \"Dessert\",\n        \"Breakfast\",\n        \"One Dish Meal\",\n    ]\n)\nactual_categories = set(df[\"category\"].dropna().unique())\nprint(\"\\nCategories not in expected set:\")\nprint(actual_categories - expected_categories)\nprint(\"Categories missing from dataset (but expected):\")\nprint(expected_categories - actual_categories)\n\nprint(\"\\nUnique servings values:\")\nprint(df[\"servings\"].unique())\nprint(df[\"servings\"].value_counts())",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VALIDATE CATEGORICAL COLUMNS\n",
    "# ============================================================================\n",
    "# Define expected categories for validation\n",
    "expected_categories = {\n",
    "    \"Lunch/Snacks\",\n",
    "    \"Beverages\",\n",
    "    \"Potato\",\n",
    "    \"Vegetable\",\n",
    "    \"Meat\",\n",
    "    \"Chicken\",\n",
    "    \"Pork\",\n",
    "    \"Dessert\",\n",
    "    \"Breakfast\",\n",
    "    \"One Dish Meal\",\n",
    "}\n",
    "\n",
    "# Validate categorical columns\n",
    "explorer.validate_categorical_columns(df, expected_categories=expected_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8500b59-4c2b-496f-bba0-9155bc35f614",
   "metadata": {},
   "source": [
    "**Validation and cleaning of servings and category**\n",
    "- There is no particularly uneven distribution across the categories. The most frequent is \"breakfast\", the least frequent is \"One Dish Meal\". However, we can see that one category is not supposed to be there, which is \"Chicken Breast\". All cases where category = \"Chicken Breast\" will be recoded to \"Chicken\". This will make \"Chicken\" the new most frequent category.\n",
    "- The column \"servings\" is supposed to be numerical, but it has 2 text unique values in 3 observations. They are \"4 as a snack\" and \"6 as a snack\". They will respectively be recoded to 4 and 6, and the column converted to int dtype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4724188-570a-4565-b447-b82ab6a9f068",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 59,
    "lastExecutedAt": 1759768148758,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Recode Chicken Breast into Chicken and check that no further anomalies are present\ndf[\"category\"] = df[\"category\"].replace({\"Chicken Breast\": \"Chicken\"})\nexpected_categories = set(\n    [\n        \"Lunch/Snacks\",\n        \"Beverages\",\n        \"Potato\",\n        \"Vegetable\",\n        \"Meat\",\n        \"Chicken\",\n        \"Pork\",\n        \"Dessert\",\n        \"Breakfast\",\n        \"One Dish Meal\",\n    ]\n)\nactual_categories = set(df[\"category\"].dropna().unique())\nprint(\"\\nCategories not in expected set:\")\nprint(actual_categories - expected_categories)\nprint(\"Categories missing from dataset (but expected):\")\nprint(expected_categories - actual_categories)\ndf[\"category\"] = df[\"category\"].astype(\"string\")\n# Apply the function to the servings column\ndf[\"servings_clean\"] = df[\"servings\"].apply(extract_number)\n# Now replace the old column\ndf[\"servings\"] = df[\"servings_clean\"]\ndf.drop(columns=[\"servings_clean\"], inplace=True)\n# Ensure the column is int type if all entries are now numeric\ndf[\"servings\"] = pd.to_numeric(df[\"servings\"], errors=\"coerce\").astype(\"Int64\")\nprint(\"\\nServings value counts:\")\nprint(df[\"servings\"].value_counts().sort_index())\n# Recode the target columns\ndf[target_col] = df[target_col].astype(str).str.strip().str.lower() == \"high\"\nprint(f\"\\nUnique {target_col} values:\")\nprint(df[target_col].unique())\nprint(df[target_col].value_counts())",
    "outputsMetadata": {
     "0": {
      "height": 395,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEAN AND VALIDATE DATA USING DATAVALIDATOR CLASS\n",
    "# ============================================================================\n",
    "# Initialize the DataValidator with our dataset\n",
    "validator = DataValidator(df)\n",
    "\n",
    "# Perform validation and cleaning\n",
    "# This handles: category standardization, servings extraction, and target recoding\n",
    "df_cleaned = validator.validate_and_clean()\n",
    "\n",
    "# Display results after cleaning\n",
    "explorer.display_cleaning_results(\n",
    "    df_cleaned, target_col=target_col, expected_categories=expected_categories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1cfbe3-8f3e-48c8-8bfb-1961202df12d",
   "metadata": {},
   "source": [
    "**Declared changes on our target variable \"high_traffic\" and category have been applied**\n",
    "- high_traffic changed to a boolean column where missings are False and values \"High\" are True\n",
    "- the category \"Chicken Breast\" has been changed to \"Chicken\" and no further missings, or unexpected categories are present\n",
    "- the mixed type issue of the column \"servings\" has been solved changing the values \"4 as a snack\" and \"6 as a snack\" into 4 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbb379-0edb-41f9-a3f9-603e6dba75cc",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1759768148809,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "df_missing = df[df.isnull().any(axis=1)]\nprint(df_missing.head(10))",
    "outputsMetadata": {
     "0": {
      "height": 521,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXAMINE ROWS WITH MISSING VALUES\n",
    "# ============================================================================\n",
    "# Display rows that still have missing values\n",
    "df_missing = explorer.display_missing_rows(df_cleaned, num_rows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05e852-e475-470b-8c9f-665b7c90f2ef",
   "metadata": {},
   "source": [
    "**Dealing with missings in the numerical columns**\n",
    "\n",
    "As suspected, the missings in calories, carbohydrate, sugar, and protein are missing in all 4 columns. \n",
    "Since there are a few missing values, dropping them would result in prediction power loss and potentially also in selection bias. To reduce the probability of selection bias and protect the prediction power of our data I will impute those missing values using the mean of their respective columns. \n",
    "I suspect a correlation with the column \"category\", so I will test this hypothesis using ANOVA test (numerical columns with categorical columns need ANOVA). If these columns show correlation with \"category\" I will impute using the clustered mean by 'category'. I will also do an ANOVA test with the column servings (please remember, I treat servings as categorical because it has few unique values), but since the nutritional values that we have are par serving, I do not expect a significant correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c12929-b42e-461b-ac98-601453d517ad",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1759768148858,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(\"ANOVA test for nutritional numerical columns with the column category\")\nanova_test_multiple(df=df, num_cols=num_cols, cat_col=\"category\")\nprint(\"\\nANOVA test for nutritional numerical columns with the column servings\")\nanova_test_multiple(df=df, num_cols=num_cols, cat_col=\"servings\")",
    "outputsMetadata": {
     "0": {
      "height": 248,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PERFORM ANOVA TESTS FOR FEATURE CORRELATION\n",
    "# ============================================================================\n",
    "# Test correlation between numerical features and category\n",
    "print(\"ANOVA test for nutritional numerical columns with the column category\")\n",
    "StatisticalTester.anova_test_multiple(df=df_cleaned, num_cols=num_cols, cat_col=\"category\")\n",
    "\n",
    "# Test correlation between numerical features and servings\n",
    "print(\"\\nANOVA test for nutritional numerical columns with the column servings\")\n",
    "StatisticalTester.anova_test_multiple(df=df_cleaned, num_cols=num_cols, cat_col=\"servings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850a741-41d8-458c-914e-928abde90bee",
   "metadata": {},
   "source": [
    "**My expectations are correct**\n",
    "The numerical columns are correlated with the category, but not with servings. \n",
    "- I proceed with the imputation of the mean of the respective columns grouping the observations by their value in the column 'category'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e2ec9-5a83-4e57-a569-861ad59c87dc",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 53,
    "lastExecutedAt": 1759768148911,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Select only relevant columns for imputation\nto_impute = num_cols  # ['calories', 'carbohydrate', 'sugar', 'protein']\n\n# Mean imputation within each category\nimputer = SimpleImputer(strategy=\"mean\")\n\ndf_imputed = df.copy()\n\nfor col in to_impute:\n    df_imputed[col] = df_imputed.groupby([\"category\"])[col].transform(\n        lambda x: x.fillna(x.mean())\n    )\n    df_imputed[col] = df_imputed[col].round(2)\n\nfor col in num_cols:\n    print(f\"===Data distribution in dataframe with imputed values for {col}===\")\n    print(\n        f\"{col}: min = {df_imputed[col].min()}, max = {df_imputed[col].max()}, mean = {df_imputed[col].mean()}, median = {df_imputed[col].median()}\"\n    )\n    print(f\"===Data distribution in dataframe with missing values for {col}===\")\n    print(\n        f\"{col}: min = {df[col].min()}, max = {df[col].max()}, mean = {df[col].mean()}, median = {df[col].median()}\"\n    )",
    "outputsMetadata": {
     "0": {
      "height": 560,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPUTE MISSING NUMERICAL VALUES\n",
    "# ============================================================================\n",
    "# Use DataValidator's imputation method to fill missing values with\n",
    "# category-wise means. This preserves the distribution within each category.\n",
    "df_imputed = validator.impute_numerical_values(num_cols=num_cols, group_by_col=\"category\")\n",
    "\n",
    "# Compare statistics before and after imputation to ensure no bias introduced\n",
    "for col in num_cols:\n",
    "    print(f\"===Data distribution in dataframe with imputed values for {col}===\")\n",
    "    print(\n",
    "        f\"{col}: min = {df_imputed[col].min()}, max = {df_imputed[col].max()}, \"\n",
    "        f\"mean = {df_imputed[col].mean()}, median = {df_imputed[col].median()}\"\n",
    "    )\n",
    "    print(f\"===Data distribution in dataframe with missing values for {col}===\")\n",
    "    print(\n",
    "        f\"{col}: min = {df_cleaned[col].min()}, max = {df_cleaned[col].max()}, \"\n",
    "        f\"mean = {df_cleaned[col].mean()}, median = {df_cleaned[col].median()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b1e2b-8770-40bc-bbf8-c42d5d352597",
   "metadata": {},
   "source": [
    "Very good! The main distribution statistics of the imputed columns did not have any big change. So, no bias has been introduced as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ada83a-d5e9-44e4-a7de-4b6916eed64b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1759768148962,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(\"\\n=== Missing Values Imputed Data ===\")\nprint(df_imputed.isnull().sum())",
    "outputsMetadata": {
     "0": {
      "height": 248,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VERIFY NO MISSING VALUES AFTER IMPUTATION\n",
    "# ============================================================================\n",
    "# Confirm that imputation was successful and dataset is complete\n",
    "print(\"\\n=== Missing Values After Imputation ===\")\n",
    "print(df_imputed.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284cb6c8-4a91-48b2-a67b-d50726d5b57d",
   "metadata": {},
   "source": [
    "Data does not have anymore missing. Data is now ready for exploratory analysis.\n",
    "\n",
    "I now can go ahead with the exploratory analysis\n",
    "\n",
    "____________________________________________________________________________________________________\n",
    "\n",
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d5a57d-7e4b-4846-8b53-e7349e1bd1eb",
   "metadata": {},
   "source": [
    "**Distribution Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1114be1c-71c3-4264-8a56-558adb539bdd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1871,
    "lastExecutedAt": 1759768150833,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Distribution analysis\nplot_histogram(data=df_imputed, column=\"calories\", bins=30)\nplot_histogram(data=df_imputed, column=\"carbohydrate\", bins=30)\nplot_histogram(data=df_imputed, column=\"sugar\", bins=30)\nplot_histogram(data=df_imputed, column=\"protein\", bins=30)\nplot_barplot(data=df_imputed, column=\"servings\")\nplot_barplot(data=df_imputed, column=\"category\")\nplot_barplot(data=df_imputed, column=\"high_traffic\")\npercentage_true = df_imputed[\"high_traffic\"].mean() * 100\nprint(f\"Percentage of True: {percentage_true:.2f}%\")",
    "outputsMetadata": {
     "7": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPLORATORY ANALYSIS - UNIVARIATE DISTRIBUTIONS\n",
    "# ============================================================================\n",
    "# Visualize distributions of numerical features\n",
    "visualizer = DataVisualizer()\n",
    "\n",
    "# Plot histograms for numerical columns\n",
    "visualizer.plot_histogram(data=df_imputed, column=\"calories\", bins=30)\n",
    "visualizer.plot_histogram(data=df_imputed, column=\"carbohydrate\", bins=30)\n",
    "visualizer.plot_histogram(data=df_imputed, column=\"sugar\", bins=30)\n",
    "visualizer.plot_histogram(data=df_imputed, column=\"protein\", bins=30)\n",
    "\n",
    "# Plot barplots for categorical columns\n",
    "visualizer.plot_barplot(data=df_imputed, column=\"servings\")\n",
    "visualizer.plot_barplot(data=df_imputed, column=\"category\")\n",
    "visualizer.plot_barplot(data=df_imputed, column=\"high_traffic\")\n",
    "\n",
    "# Calculate and display percentage of positive class\n",
    "percentage_true = df_imputed[\"high_traffic\"].mean() * 100\n",
    "print(f\"Percentage of True: {percentage_true:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329a156-68c6-4e8f-b2d9-0da5c3421d82",
   "metadata": {},
   "source": [
    "**Comments on distribution analysis**\n",
    "I created two types of graphics to visualize the distribution of the values in the columns. I used histogram for analysing the distribution of numerical columns and a horizontal barplot to visualize frequencies in categorical columns (as said at the beginning, servings has only 4 unique values, I treat it as a categorical).\n",
    "- Interestingly, we can see how all the 4 numerical columns seem to show a right-skewed distribution, with the presence of some non-critical outliers\n",
    "- The analysis of the categorical columns highlights how the most frequent serving is 4, the most frequent recipe category is chicken, and there are more True than False in the data.\n",
    "\n",
    "**Very important: True occurs in 60.61% of the cases**, this means that the current human selection of the recipe to put in first page is able to detect a recipe with high web traffic interest in 60.61% of the cases.\n",
    "I will now analyze correlations between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1be3b6-3601-470b-968e-3240a7df9c54",
   "metadata": {},
   "source": [
    "**Simple correlation analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059583a-f8e2-4169-b5b1-9e970bf75e34",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 414,
    "lastExecutedAt": 1759768151247,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "plot_lower_triangle_heatmap(df=df_imputed, id_col=\"recipe\", exclude_cols=[\"category\"])\nprint(\"ANOVA test for imputed data\")\nanova_test_multiple(df=df_imputed, num_cols=num_cols, cat_col=\"category\")",
    "outputsMetadata": {
     "1": {
      "height": 122,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SIMPLE CORRELATION ANALYSIS - HEATMAP AND ANOVA\n",
    "# ============================================================================\n",
    "# Create lower triangle correlation heatmap\n",
    "visualizer.plot_lower_triangle_heatmap(df=df_imputed, id_col=\"recipe\", exclude_cols=[\"category\"])\n",
    "\n",
    "# Perform ANOVA tests to measure association with category\n",
    "print(\"ANOVA test for imputed data\")\n",
    "StatisticalTester.anova_test_multiple(df=df_imputed, num_cols=num_cols, cat_col=\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8407bb8-2670-4515-8653-af861e5e6980",
   "metadata": {},
   "source": [
    "**First comments on correlation analysis**\n",
    "- Correlations between the variables look all very low/modest, nothing interesting to note in the heatmap\n",
    "- The ANOVA test stated a strong correlation between the 4 nutrional numerical columns and the category. Keep in mind for multicollinearity risks in linear models.\n",
    "\n",
    "**Scatterplots between numerical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa07d37-666a-42e4-b335-f1fc12065bbb",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1327,
    "lastExecutedAt": 1759768152574,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "scatterplot_with_reg_line(data=df_imputed, x_col=\"calories\", y_col=\"protein\")\nscatterplot_with_reg_line(data=df_imputed, x_col=\"calories\", y_col=\"carbohydrate\")\nscatterplot_with_reg_line(data=df_imputed, x_col=\"calories\", y_col=\"sugar\")\nscatterplot_with_reg_line(data=df_imputed, x_col=\"sugar\", y_col=\"protein\")\nscatterplot_with_reg_line(data=df_imputed, x_col=\"sugar\", y_col=\"carbohydrate\")\nscatterplot_with_reg_line(data=df_imputed, x_col=\"carbohydrate\", y_col=\"protein\")"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PAIRWISE SCATTERPLOTS WITH REGRESSION LINES\n",
    "# ============================================================================\n",
    "# Visualize relationships between numerical features\n",
    "visualizer.scatterplot_with_reg_line(data=df_imputed, x_col=\"calories\", y_col=\"protein\")\n",
    "visualizer.scatterplot_with_reg_line(data=df_imputed, x_col=\"calories\", y_col=\"carbohydrate\")\n",
    "visualizer.scatterplot_with_reg_line(data=df_imputed, x_col=\"calories\", y_col=\"sugar\")\n",
    "visualizer.scatterplot_with_reg_line(data=df_imputed, x_col=\"sugar\", y_col=\"protein\")\n",
    "visualizer.scatterplot_with_reg_line(data=df_imputed, x_col=\"sugar\", y_col=\"carbohydrate\")\n",
    "visualizer.scatterplot_with_reg_line(data=df_imputed, x_col=\"carbohydrate\", y_col=\"protein\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b1e36a-5454-4450-a0ec-51fc24c5adf4",
   "metadata": {},
   "source": [
    "- Surprisingly the nutrition numerical columns show very modest correlation with each other. If they are used as features it should be done with a lot of attention because they might not add anything meaningful.\n",
    "\n",
    "**Following analysis of the categorical columns correlation with our target.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9065516-6d40-49db-a9a3-042fefa13ba2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 879,
    "lastExecutedAt": 1759768153453,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "chi_squared_test(df=df_imputed, cat_col=\"category\")\nplot_category_high_traffic(df=df_imputed, cat_col=\"category\")\nchi_squared_test(df=df_imputed, cat_col=\"servings\")\nplot_category_high_traffic(df=df_imputed, cat_col=\"servings\")\nchi_squared_test(df=df_imputed, cat_col=\"servings\", target_col=\"category\")",
    "outputsMetadata": {
     "0": {
      "height": 101,
      "type": "stream"
     },
     "2": {
      "height": 101,
      "type": "stream"
     },
     "4": {
      "height": 101,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CATEGORICAL FEATURES vs TARGET VARIABLE ANALYSIS\n",
    "# ============================================================================\n",
    "# Test association between category and target using chi-squared test\n",
    "StatisticalTester.chi_squared_test(df=df_imputed, cat_col=\"category\")\n",
    "visualizer.plot_category_high_traffic(df=df_imputed, cat_col=\"category\")\n",
    "\n",
    "# Test association between servings and target\n",
    "StatisticalTester.chi_squared_test(df=df_imputed, cat_col=\"servings\")\n",
    "visualizer.plot_category_high_traffic(df=df_imputed, cat_col=\"servings\")\n",
    "\n",
    "# Test association between servings and category\n",
    "StatisticalTester.chi_squared_test(df=df_imputed, cat_col=\"servings\", target_col=\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b32c1b-05b4-44ac-8328-2e1e527cc84a",
   "metadata": {},
   "source": [
    "**Bingo! I have found the best candidate for being the main feature to predict the classification high_traffic**\n",
    "\n",
    "I used stacked bar graphs and chi2 test to test correlation between the categorical variables and the boolean outcome in my data.\n",
    "\n",
    "- The column 'category' show a very high correlation with the outcome variable high_traffic. \n",
    "- We can see that the most popular recipe are in the categories vegetable, potato, and pork, while the least popular are beverages, breakfast, and chicken.\n",
    "- The variable servings on the contrary does not show any correlation neither with categories nor with the traffic outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5517ee-f90e-47cd-a8cb-a9f4485a3b0c",
   "metadata": {},
   "source": [
    "**Let's analyze the numerical features correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb7a52-5025-4c8e-971c-1457f6a23ba4",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 877,
    "lastExecutedAt": 1759768154330,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "draw_boxplot(feature=\"calories\", target=\"high_traffic\", data=df_imputed)\ndraw_boxplot(feature=\"carbohydrate\", target=\"high_traffic\", data=df_imputed)\ndraw_boxplot(feature=\"sugar\", target=\"high_traffic\", data=df_imputed)\ndraw_boxplot(feature=\"protein\", target=\"high_traffic\", data=df_imputed)\nanova_test_multiple(df=df_imputed, num_cols=num_cols, cat_col=\"high_traffic\")",
    "outputsMetadata": {
     "4": {
      "height": 101,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NUMERICAL FEATURES vs TARGET VARIABLE ANALYSIS\n",
    "# ============================================================================\n",
    "# Visualize distributions of numerical features across target groups using boxplots\n",
    "visualizer.draw_boxplot(feature=\"calories\", target=\"high_traffic\", data=df_imputed)\n",
    "visualizer.draw_boxplot(feature=\"carbohydrate\", target=\"high_traffic\", data=df_imputed)\n",
    "visualizer.draw_boxplot(feature=\"sugar\", target=\"high_traffic\", data=df_imputed)\n",
    "visualizer.draw_boxplot(feature=\"protein\", target=\"high_traffic\", data=df_imputed)\n",
    "\n",
    "# Perform ANOVA tests to measure statistical significance of differences\n",
    "StatisticalTester.anova_test_multiple(df=df_imputed, num_cols=num_cols, cat_col=\"high_traffic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643e567-462d-4974-a1e0-13e1b09e4b92",
   "metadata": {},
   "source": [
    "**First impressions**\n",
    "\n",
    "I use boxplot visualizations to check graphically the correlation between numerical columns and boolean,none has been observed, but this graphical inspection is not able to detect non-linear patterns. The ANOVA test detect a not super strong, but significant correlation of the numerical columns with the outcome variable. I will investigate non-linear patterns with a scatterplot with jitter using binned means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36897421-c0a3-4641-9021-5229d87b3880",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1456,
    "lastExecutedAt": 1759768155786,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "scatterplot_with_binned_means(\n    data=df_imputed, num_col=\"calories\", target_col=\"high_traffic\"\n)\nscatterplot_with_binned_means(\n    data=df_imputed, num_col=\"carbohydrate\", target_col=\"high_traffic\"\n)\nscatterplot_with_binned_means(\n    data=df_imputed, num_col=\"sugar\", target_col=\"high_traffic\"\n)\nscatterplot_with_binned_means(\n    data=df_imputed, num_col=\"protein\", target_col=\"high_traffic\"\n)"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NON-LINEAR RELATIONSHIP ANALYSIS WITH BINNED MEANS\n",
    "# ============================================================================\n",
    "# Create scatterplots with binned means to detect non-linear patterns\n",
    "visualizer.scatterplot_with_binned_means(\n",
    "    data=df_imputed, num_col=\"calories\", target_col=\"high_traffic\"\n",
    ")\n",
    "visualizer.scatterplot_with_binned_means(\n",
    "    data=df_imputed, num_col=\"carbohydrate\", target_col=\"high_traffic\"\n",
    ")\n",
    "visualizer.scatterplot_with_binned_means(\n",
    "    data=df_imputed, num_col=\"sugar\", target_col=\"high_traffic\"\n",
    ")\n",
    "visualizer.scatterplot_with_binned_means(\n",
    "    data=df_imputed, num_col=\"protein\", target_col=\"high_traffic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce37b6df-dbb6-41ff-82ec-25bf7465f301",
   "metadata": {},
   "source": [
    "**Possible small but significant non-linear correlation of the nutrional numerical columns**\n",
    "\n",
    "It seems to exist a non-linear correlation between the nutritional numerical variables calories, carbohydrate, and sugar with the outcome. \n",
    "I want to double-check if this apparent non-linear relationship is driven somehow by the presence of outliers.\n",
    "\n",
    "**Outliers analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c063a-ded4-4b59-bda8-23ce0f846751",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 104,
    "lastExecutedAt": 1759768155890,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "for col in num_cols:\n    # Group by 'category' and compute z-score within each group\n    df_imputed[f\"{col}_zscore\"] = df_imputed.groupby(\"category\")[col].transform(zscore)\n\noutliers_calories = df_imputed.loc[np.abs(df_imputed[\"calories_zscore\"]) > 3]\noutliers_carbohydrate = df_imputed.loc[np.abs(df_imputed[\"carbohydrate_zscore\"]) > 3]\noutliers_sugar = df_imputed.loc[np.abs(df_imputed[\"sugar_zscore\"]) > 3]\noutliers_protein = df_imputed.loc[np.abs(df_imputed[\"protein_zscore\"]) > 3]\n\nprint(outliers_calories[num_cols + [\"high_traffic\", \"category\"]].head(25))\nprint(outliers_carbohydrate[num_cols + [\"high_traffic\", \"category\"]].head(25))\nprint(outliers_sugar[num_cols + [\"high_traffic\", \"category\"]].head(25))\nprint(outliers_protein[num_cols + [\"high_traffic\", \"category\"]].head(25))",
    "outputsMetadata": {
     "0": {
      "height": 506,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OUTLIER DETECTION USING Z-SCORE METHOD\n",
    "# ============================================================================\n",
    "# Detect outliers within each category using z-score method\n",
    "# This helps identify extreme values that might affect model training\n",
    "for col in num_cols:\n",
    "    # Calculate z-scores within each category\n",
    "    df_imputed[f\"{col}_zscore\"] = df_imputed.groupby(\"category\")[col].transform(zscore)\n",
    "\n",
    "# Identify observations with z-score > 3 (extreme outliers) for each numerical feature\n",
    "outliers_calories = df_imputed.loc[np.abs(df_imputed[\"calories_zscore\"]) > 3]\n",
    "outliers_carbohydrate = df_imputed.loc[np.abs(df_imputed[\"carbohydrate_zscore\"]) > 3]\n",
    "outliers_sugar = df_imputed.loc[np.abs(df_imputed[\"sugar_zscore\"]) > 3]\n",
    "outliers_protein = df_imputed.loc[np.abs(df_imputed[\"protein_zscore\"]) > 3]\n",
    "\n",
    "print(\"Outliers in calories:\")\n",
    "print(outliers_calories[num_cols + [\"high_traffic\", \"category\"]].head(25))\n",
    "print(\"\\nOutliers in carbohydrate:\")\n",
    "print(outliers_carbohydrate[num_cols + [\"high_traffic\", \"category\"]].head(25))\n",
    "print(\"\\nOutliers in sugar:\")\n",
    "print(outliers_sugar[num_cols + [\"high_traffic\", \"category\"]].head(25))\n",
    "print(\"\\nOutliers in protein:\")\n",
    "print(outliers_protein[num_cols + [\"high_traffic\", \"category\"]].head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a815da-7974-42f7-80d6-10377a70c45c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1759768155942,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(\"Test if outliers are correlated with our outcome variable\")\nanova_test_multiple(df=outliers_calories, num_cols=[\"calories\"], cat_col=\"high_traffic\")\nanova_test_multiple(\n    df=outliers_carbohydrate, num_cols=[\"carbohydrate\"], cat_col=\"high_traffic\"\n)\nanova_test_multiple(df=outliers_sugar, num_cols=[\"sugar\"], cat_col=\"high_traffic\")",
    "outputsMetadata": {
     "0": {
      "height": 101,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEST IF OUTLIERS CORRELATE WITH TARGET VARIABLE\n",
    "# ============================================================================\n",
    "# Perform ANOVA tests on outliers to check if they have specific relationship with target\n",
    "print(\"Test if outliers are correlated with our outcome variable\")\n",
    "StatisticalTester.anova_test_multiple(\n",
    "    df=outliers_calories, num_cols=[\"calories\"], cat_col=\"high_traffic\"\n",
    ")\n",
    "StatisticalTester.anova_test_multiple(\n",
    "    df=outliers_carbohydrate, num_cols=[\"carbohydrate\"], cat_col=\"high_traffic\"\n",
    ")\n",
    "StatisticalTester.anova_test_multiple(df=outliers_sugar, num_cols=[\"sugar\"], cat_col=\"high_traffic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054858ad-0b9d-4b78-a795-e248d1917881",
   "metadata": {},
   "source": [
    "**Results of the outliers analysis**\n",
    "\n",
    "There are a few non-critical outliers, but they are not correlated with the outcome high_traffic, so they won't disturb the models.\n",
    "\n",
    "It is now the time of planning the model development and feature selection and engineering.\n",
    "\n",
    "**Small summary**\n",
    "\n",
    "It must be noted that:\n",
    "- protein and servings did not show any significant correlation pattern with the outcome\n",
    "- category is the only column that show a very strong and clear correlation with the high_traffic column\n",
    "- the other numerical columns calories, sugar, carbohydrate are highly correlated with the variable category which will be the main X in my models. This might create multicollinearity issues in linear models. \n",
    "- the numerical columns presented 52 outliers which have been imputed, and they have shown not clear correlation patterns\n",
    "- **This task is a classification task because we need to predict if the recipes show high traffic or not**.\n",
    "\n",
    "# Model Development\n",
    "\n",
    "**Action plan**\n",
    "\n",
    "I will train two models to correctly **classify** recipes. The first model will use Logistic Regression model and it will contain only an encoded version of the category column because linear models are not very robust to detect non-linear relationship and they are sensible to multicollinearity.\n",
    "\n",
    "The second comparison model will be a Random Forest Classifier, with it I will try to model the modest non-linear relationship (If relevant) of the columns carbohydrate, sugar, and calories. If the non-linear part of the relation is relevant, it will improve the performance of the evaluation metrics. In addition, this model is robust to multicollinearity, so no problems will arise from the strong correlation between these numerical columns and the variable category.\n",
    "\n",
    "I will then evaluate both of them considering mostly the recall and the precision because our business problem regards mostly being able to detect correctly all the popular recipes.\n",
    "\n",
    "**Disclaimer**\n",
    "I used RandomizedSearchCV for both of the Random Forest and Logistic Regression to find the best combination of fine-tuning parameters given ranges and possibilities that are considered state-of-the-art (or standard) for this type of classification problem. However, for reproducibility of the results, I only report here the best-fitted model; I decided to follow this strategy because running the RandomizedSearchCV will always result in slight and negligible differences, but still might damage perfect reproducibility.\n",
    "\n",
    "For transparency reasons, I leave as commented lines the code for the Randomized Search.\n",
    "\n",
    "Now is the time of **feature engineering and train test splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d29137-6289-4373-a0ba-ee4f309613f8",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 53,
    "lastExecutedAt": 1759768155995,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Relevant columns are selected and double chekced for missings\nX = df_imputed[[\"calories\", \"sugar\", \"carbohydrate\", \"category\"]].copy()\ny = df_imputed[\"high_traffic\"].copy()\nprint(X.info())\nprint(y.info())\nprint(\"\\nUnique category values:\")\nprint(X[\"category\"].unique())\nprint(X[\"category\"].value_counts())",
    "outputsMetadata": {
     "0": {
      "height": 506,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE SELECTION FOR MODEL DEVELOPMENT\n",
    "# ============================================================================\n",
    "# Select relevant features for model training\n",
    "# Features include numerical nutritional data and categorical recipe type\n",
    "X = df_imputed[[\"calories\", \"sugar\", \"carbohydrate\", \"category\"]].copy()\n",
    "y = df_imputed[\"high_traffic\"].copy()\n",
    "\n",
    "# Verify data integrity before modeling\n",
    "print(X.info())\n",
    "print(y.info())\n",
    "print(\"\\nUnique category values:\")\n",
    "print(X[\"category\"].unique())\n",
    "print(X[\"category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45181289-4334-47d9-bab1-bef8bfccc21e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 53,
    "lastExecutedAt": 1759768156048,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "for col in [\"calories\", \"sugar\", \"carbohydrate\"]:\n    X[col + \"_log\"] = np.log1p(X[col])    \n# I encode the categories in category column to be handled by the algorhithms. This is the last step of the feature enginnering\nX_transformed = pd.get_dummies(X, columns=[\"category\"], drop_first=True)\nX_transformed.head()",
    "outputsMetadata": {
     "0": {
      "height": 550,
      "tableState": {},
      "type": "dataFrame"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "# Prepare features for modeling with log transformation and one-hot encoding\n",
    "X_transformed = fe.prepare_features_for_modeling(\n",
    "    X, numerical_cols=[\"calories\", \"sugar\", \"carbohydrate\"], categorical_cols=[\"category\"]\n",
    ")\n",
    "\n",
    "# Display first 5 rows of feature-engineered data\n",
    "print(\"Feature-engineered data (first 5 rows):\")\n",
    "print(X_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e391734-6249-45de-8ab2-0466891bc598",
   "metadata": {},
   "source": [
    "**Summary of feature engineering**\n",
    "In the feature engineering part I: \n",
    "- selected all relevant columns\n",
    "- created log functions of the numerical features to penalize the weight of outliers\n",
    "- one hot encoded the category column\n",
    "\n",
    "I will now split the data in train and test sets and create the final training and test data for my two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5933af-3de6-4ef7-ad2f-bc009c65d402",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1759768156098,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "X_train, X_test, y_train, y_test = train_test_split(\n    X_transformed, y, train_size=0.75, random_state=42, shuffle=True\n)\nnutrition_columns_raw = [\"calories\", \"sugar\", \"carbohydrate\"]\nnutrition_columns_logs = [\"calories_log\", \"sugar_log\", \"carbohydrate_log\"]\n\n\nX_train_without_nutr = X_train.drop(\n    columns=nutrition_columns_raw\n    + nutrition_columns_logs\n).copy()\nX_test_without_nutr = X_test.drop(\n    columns=nutrition_columns_raw\n    + nutrition_columns_logs\n).copy()\nX_train_with_log = X_train.drop(columns=nutrition_columns_raw).copy()\nX_test_with_log = X_test.drop(columns=nutrition_columns_raw).copy()"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN-TEST SPLIT AND DATA PREPARATION\n",
    "# ============================================================================\n",
    "# Split data into training (75%) and testing (25%) sets with random seed for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_transformed, y, train_size=0.75, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Define column names for different feature sets\n",
    "nutrition_columns_raw = [\"calories\", \"sugar\", \"carbohydrate\"]\n",
    "nutrition_columns_logs = [\"calories_log\", \"sugar_log\", \"carbohydrate_log\"]\n",
    "\n",
    "# Prepare different dataset configurations for different models\n",
    "(X_train_without_nutr, X_test_without_nutr, X_train_with_log, X_test_with_log) = (\n",
    "    fe.prepare_train_test_datasets(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        numerical_cols_raw=nutrition_columns_raw,\n",
    "        numerical_cols_logs=nutrition_columns_logs,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e449b91-6d8f-424d-a5df-43e72f403e19",
   "metadata": {},
   "source": [
    "## Logistic Regression - Training\n",
    "\n",
    "The split has been done, let's start training the logistic regression benchmark model.\n",
    "I used f1 as scoring criteria in the Randomized Search because I am more concerned about finding all positives\n",
    "with the lowest error possible in predicting positives. I used broadly accepted ranges for the parameters fine-tuning of such a model.\n",
    "I only use categories as \"X\" to avoid multicollinearity since the numerical columns are strongly correlated with the category as we saw in the exploratory analysis.\n",
    "\n",
    "Remember, the commented lines are for the Randomized Search, but I want reproducibility, so after I found out the best parameters I just commented the lines and hard-coded the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5a708-390a-4e0b-b720-0e528269e7b0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 185,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOGISTIC REGRESSION MODEL - TRAINING\n",
    "# ============================================================================\n",
    "# Initialize and train Logistic Regression model using category features only\n",
    "# This avoids multicollinearity while providing a simple, interpretable baseline\n",
    "logreg_wrapper = LogisticRegressionModel(\n",
    "    solver=\"lbfgs\", penalty=\"l2\", l1_ratio=0.75, C=109.85411419875572, max_iter=10000\n",
    ")\n",
    "\n",
    "# Fit model on training data\n",
    "logreg_wrapper.fit(X_train_without_nutr, y_train)\n",
    "\n",
    "# Make predictions on both training and test sets\n",
    "y_pred_logreg_train = logreg_wrapper.predict(X_train_without_nutr)\n",
    "y_proba_train = logreg_wrapper.predict_proba(X_train_without_nutr)[:, 1]\n",
    "\n",
    "y_pred_logreg_test = logreg_wrapper.predict(X_test_without_nutr)\n",
    "y_proba_test = logreg_wrapper.predict_proba(X_test_without_nutr)[:, 1]\n",
    "\n",
    "# Calculate evaluation metrics for training set\n",
    "metrics_logreg_train = ModelEvaluator.calculate_metrics(y_train, y_pred_logreg_train, y_proba_train)\n",
    "\n",
    "# Calculate evaluation metrics for test set\n",
    "metrics_logreg_test = ModelEvaluator.calculate_metrics(y_test, y_pred_logreg_test, y_proba_test)\n",
    "\n",
    "# Print detailed evaluation report\n",
    "ModelEvaluator.print_metrics_report(\n",
    "    \"Logistic Regression\", metrics_logreg_train, metrics_logreg_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcb6b70-aa11-4a63-b370-ef088df3ed56",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 572,
    "lastExecutedAt": 1759768156726,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "plot_logistic_coefficients(logreg, X_train_without_nutr.columns)\nplot_confusion(\n    y_pred=y_pred_logreg_without_nutr,\n    y_true=y_test,\n    title=\"Logistic Regression Confusion Matrix\",\n)\nfeatures_table = [\"Intercept\"] + list(X_train_without_nutr.columns)\ntable = logistic_regression_table(logreg, features_table)\nprint(table)",
    "outputsMetadata": {
     "2": {
      "height": 248,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOGISTIC REGRESSION MODEL - VISUALIZATION AND INTERPRETATION\n",
    "# ============================================================================\n",
    "# Visualize the coefficients of the logistic regression model\n",
    "model_viz = ModelVisualizer()\n",
    "model_viz.plot_logistic_coefficients(logreg_wrapper.model, X_train_without_nutr.columns)\n",
    "\n",
    "# Plot confusion matrix for test set predictions\n",
    "visualizer.plot_confusion(\n",
    "    y_pred=y_pred_logreg_test,\n",
    "    y_true=y_test,\n",
    "    title=\"Logistic Regression Confusion Matrix\",\n",
    ")\n",
    "\n",
    "# Create and display table of model coefficients for interpretation\n",
    "features_table = list(X_train_without_nutr.columns)\n",
    "table = ModelInterpreter.logistic_regression_table(logreg_wrapper.model, features_table)\n",
    "print(\"Logistic Regression Coefficients Table:\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa79cd-9493-4f40-b1c2-7a8df67d28fd",
   "metadata": {},
   "source": [
    "## Logistic Regression - Initial model evaluation\n",
    "\n",
    "Very good results have been reached. The recall on the test set is 0.812, and the precision on the test set is 0.817. The evaluation metrics between training and test set are very similar, actually slightly better on the test set, indicating that there is no overfitting. This model satisfies the business request, but I want to try a more sophisticated model using Random Forest, and more features to model non-linear relationship observed in the numerical features without to worry too much about multicollinearity.\n",
    "\n",
    "## Random Forest Classifier - Training\n",
    "\n",
    "I used f1 as scoring criteria in the Randomized Search because I am more concerned about finding all positives\n",
    "with the lowest error possible in predicting positives. I used broadly accepted ranges for the parameters fine-tuning of such a model.\n",
    "\n",
    "I also use logged numerical features because they presented a right-skewed distribution. Random Forest is more robust to multicollinearity and to non-linear relationships, so no need to worry about that.\n",
    "\n",
    "Remember, the commented lines are for the Randomized Search, but I want reproducibility, so after I found out the best parameters I just commented the lines and hard-coded the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487af5f-9c3a-4d25-9da4-2a29c52c6999",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 185,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RANDOM FOREST CLASSIFIER MODEL - TRAINING\n",
    "# ============================================================================\n",
    "# Initialize and train Random Forest model using all features including numerical\n",
    "# Random Forest is robust to multicollinearity and can capture non-linear relationships\n",
    "rf_wrapper = RandomForestModel(\n",
    "    n_estimators=10,\n",
    "    min_samples_split=15,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"log2\",\n",
    "    max_depth=None,\n",
    "    class_weight=None,\n",
    "    bootstrap=True,\n",
    ")\n",
    "\n",
    "# Fit model on training data with log-transformed numerical features\n",
    "rf_wrapper.fit(X_train_with_log, y_train)\n",
    "\n",
    "# Make predictions on both training and test sets\n",
    "y_pred_rf_train = rf_wrapper.predict(X_train_with_log)\n",
    "y_proba_rf_train = rf_wrapper.predict_proba(X_train_with_log)[:, 1]\n",
    "\n",
    "y_pred_rf_test = rf_wrapper.predict(X_test_with_log)\n",
    "y_proba_rf_test = rf_wrapper.predict_proba(X_test_with_log)[:, 1]\n",
    "\n",
    "# Calculate evaluation metrics for training set\n",
    "metrics_rf_train = ModelEvaluator.calculate_metrics(y_train, y_pred_rf_train, y_proba_rf_train)\n",
    "\n",
    "# Calculate evaluation metrics for test set\n",
    "metrics_rf_test = ModelEvaluator.calculate_metrics(y_test, y_pred_rf_test, y_proba_rf_test)\n",
    "\n",
    "# Print detailed evaluation report\n",
    "ModelEvaluator.print_metrics_report(\"Random Forest Classifier\", metrics_rf_train, metrics_rf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd36bd-0d09-4db2-b29f-b227bbfb932e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 621,
    "lastExecutedAt": 1759770466726,
    "lastExecutedByKernel": "ecfd1719-232c-4bd4-9354-4422ebdcac2b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "plot_feature_importance_rf(\n    model=rf, feature_names=X_train_with_log.columns\n)\nplot_confusion(\n    y_pred=y_pred_rf_with_log,\n    y_true=y_test,\n    title=\"Random Forest Confusion Matrix\",\n)"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RANDOM FOREST CLASSIFIER MODEL - VISUALIZATION AND INTERPRETATION\n",
    "# ============================================================================\n",
    "# Visualize feature importances from Random Forest\n",
    "model_viz.plot_feature_importance_rf(model=rf_wrapper.model, feature_names=X_train_with_log.columns)\n",
    "\n",
    "# Plot confusion matrix for test set predictions\n",
    "visualizer.plot_confusion(\n",
    "    y_pred=y_pred_rf_test,\n",
    "    y_true=y_test,\n",
    "    title=\"Random Forest Confusion Matrix\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6174db3-1ee8-4911-a445-711a25f8a8e5",
   "metadata": {},
   "source": [
    "## Random Forest Classifier - Initial model evaluation\n",
    "\n",
    "No significative improvements from the logistics regression. Actually there was a slight decrease in all the model evaluation metrics (accuracy, precision, recall, and ROC AUC score). Evaluation metrics look better on the training set, this means that the Random Forest model suffers of moderate overfitting. I am mostly focused on recall and precision and these two metrics went down in the test set in relation to the benchmark model.\n",
    "\n",
    "# Model evaluation - Summary\n",
    "\n",
    "**Comparison**\n",
    "\n",
    "Final evaluation compares only metrics obtained from the **test set** between the two models.\n",
    "\n",
    "In addition to the previously mentioned initial model evaluation,\n",
    "the baseline Logistic Regression model has correctly labeled 121/149 True observations and it erroneously labeled as positives only 27 observations. On the comparison, the Random Forest Classifier has correctly found fewer True observations, and it erroneously labeled more observations as positives as shown in the confusion matrix picture. Lastly, the logistic regression model is a very efficient and parsimonious model, in fact does not suffer of overfitting. Differently the random forest model suffers of a moderate overfitting. It is clear that the best model is the benchmark Logistic Regression model. The business should use it for selecting which recipes should go on the first page of the website to increase the web traffic. \n",
    "\n",
    "# Business metrics\n",
    "\n",
    "The two main general questions coming from business were: \n",
    "- Predict which recipes will lead to high traffic?\n",
    "- Correctly predict high traffic recipes 80% of the time?\n",
    "\n",
    "The first question refers to the recall of a classification model (modelâ€™s ability to capture all relevant positive cases). The second question refers to the precision of a classification model (It is the proportion of true positive results among all instances the model predicted as positive). Therefore is vital for the business to monitor constantly the precision and the recall of the best model, and in case of deterioration of the performances update the model. In addition, the business should monitor the coefficients of the model, as of now in the table coefficient table and visualization shown above, the categories that most are able to predict a popular recipe are pork, potato, and vegetable. The least popular are beverages, breakfast and chicken.\n",
    "\n",
    "Another metric to monitor for the business is the actual increase of web traffic. From the exploratory analysis above, I found out that the old only human based classification system was able to find the popular recipes in 60.61% of the cases. My model is able to predict with more than 80% precision what will be a popular recipe able to generate as much as 40% increase in the website. Therefore, implementing my model will lead as of now to an overall web traffic increase on average of up to 40 * precision (0.817) - 40 * 0.606 =  8.44% in relation to the currently only human-generated choice of the first page recipe. Business should monitor if after implementation of this model there is an increase of web traffic in the dimension of what I estimated. This estimation is of course to be taken as an estimation keeping all other factors constant. If the performance of this KPI should change over time the model should be updated, or the reasons of why the performance changed should be furtherly investigated.\n",
    "\n",
    "# Final summary\n",
    "\n",
    "The data has been properly cleaned and imputed to allow me to perform a proper analysis. From the exploratory analysis the main take away was the very high correlation of category in predicting whether a recipe is going to be popular or not. Two models were trained and compared. The Logistic Regression model outperforms the Random Forest Classifier by achieving higher recall and fewer false positives. Random Forest Classifier model suffers also of moderate overfitting. This makes Logistic Regression the recommended model for selecting recipes that should appear on the websiteâ€™s first page to maximize traffic.\n",
    "\n",
    "The business should monitor recall and precision, as these directly map to the companyâ€™s goals of identifying high-traffic recipes and ensuring predictions have high precision. Model coefficients should also be tracked, since key predictors of popular recipes are categories like pork, potato, and vegetable, but things can change and the model needs to stay updated with the current tastes.\n",
    "\n",
    "Additionally, the last business impact metric to evaluate is the increase in web traffic. Compared to the old human-based system (60.61% precision), the new model is estimated to increase traffic by about 8.44%, assuming other factors remain constant. Continuous monitoring of this KPI is necessary, and if results deviate, either the model must be updated or the reasons behind performance changes must be investigated.\n",
    "\n",
    "## Business Recommendations:\n",
    "\n",
    "- Deploy the Logistic Regression model for homepage recipe selection.\n",
    "- Track precision, recall, and web traffic increases monthly.\n",
    "- Review feature importances (model coefficients) for emerging popular categories. At the moment they are vegetable, pork, potato. Keep in mind that right now the recipes in the categories of beverages, chicken and breakfast are not very popular. \n",
    "- You can design an A/B test experiment to figure it out if this model really works as intended in predicting popular recipes and finally increase web traffic.\n",
    "- Update or retrain the model if KPIs change or model performance deteriorates.\n",
    "Adopting these recommendations will ensure selection precision, enable ongoing improvements, and help the business meet its traffic goals."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "high_web_traffic_recipe (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
